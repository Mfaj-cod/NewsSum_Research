data:
  train_file: data/newssumm_processed/newssumm_processed.json
  max_input_length: 4096
  max_target_length: 256
  val_split: 0.01

experiment:
  name: longt5_run_001
  output_dir: results/longt5_run_001

model:
  name: google/long-t5-tglobal-base
  family: longt5
  type: seq2seq  

system:
  device: auto

training:
  batch_size: 1
  epochs: 1
  fp16: true
  learning_rate: 2e-5
  seed: 42
  gradient_accumulation_steps: 8  # makes effective batch = 8
  max_train_samples: 100