data:
  train_file: data/newssumm_processed/newssumm_processed.json
  max_input_length: 4096
  max_target_length: 256
  val_split: 0.01

experiment:
  name: llama3_8b_run_001
  output_dir: results/llama3_8b_run_001

model:
  name: meta-llama/Meta-Llama-3-8B-Instruct
  family: llama3
  type: causal

system:
  device: auto

training:
  batch_size: 1
  epochs: 1
  fp16: true
  learning_rate: 1e-5
  seed: 42
  gradient_accumulation_steps: 8  # makes effective batch = 8
  max_train_samples: 100
